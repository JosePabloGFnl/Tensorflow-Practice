{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module6NaturalLanguageProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/Kb41GpAxotY0kLClvAWZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosePabloGFnl/Tensorflow-Practice/blob/main/Module6NaturalLanguageProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhLcgCKiLTaY"
      },
      "outputs": [],
      "source": [
        "#----NATURAL LANGUAGE PROCESSING-----\n",
        "#Natural Language Processing (or NLP for short) is a discipline in computing that deals with the \n",
        "#communication between natural (human) languages and computer languages. \n",
        "#A common example of NLP is something like spellcheck or autocomplete. Essentially NLP is the field that focuses on how \n",
        "#computers can understand and/or process natural/human languages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We will learn how to use a reccurent neural network to do the following:\n",
        "\n",
        "#Sentiment Analysis\n",
        "#Character Generation"
      ],
      "metadata": {
        "id": "m1z2HXShPefz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BAG OF WORDS TECHNIQUE\n",
        "#consult notebook for info\n",
        "\n",
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
        "  bag = {}  # stores all of the encodings and their frequency\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      encoding = vocab[word]  # get encoding from vocab\n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding = word_encoding\n",
        "      word_encoding += 1\n",
        "    \n",
        "    if encoding in bag:\n",
        "      bag[encoding] += 1\n",
        "    else:\n",
        "      bag[encoding] = 1\n",
        "  \n",
        "  return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wI5WmQGP6gp",
        "outputId": "8962e8e1-8e05-416b-fa5a-d4380fadded3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-Sentiment Analysis-\n",
        "#the process of computationally identifying and categorizing opinions expressed in a piece of text, \n",
        "#especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n",
        "\n",
        "#The example weâ€™ll use here is classifying movie reviews as either postive, negative or neutral. with already preprocessed reviews\n",
        "\n",
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584 #the 88584 word is the least common in the data set\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "v2bGwPa7Ymma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2631080-fde6-4ec1-e4b4-3f148927215e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets look at one review\n",
        "train_data[1]\n",
        "#it will print all of the words in the review"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mil8SrARalXZ",
        "outputId": "78ceb396-2878-47ac-e5ce-20ba6356c927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 194,\n",
              " 1153,\n",
              " 194,\n",
              " 8255,\n",
              " 78,\n",
              " 228,\n",
              " 5,\n",
              " 6,\n",
              " 1463,\n",
              " 4369,\n",
              " 5012,\n",
              " 134,\n",
              " 26,\n",
              " 4,\n",
              " 715,\n",
              " 8,\n",
              " 118,\n",
              " 1634,\n",
              " 14,\n",
              " 394,\n",
              " 20,\n",
              " 13,\n",
              " 119,\n",
              " 954,\n",
              " 189,\n",
              " 102,\n",
              " 5,\n",
              " 207,\n",
              " 110,\n",
              " 3103,\n",
              " 21,\n",
              " 14,\n",
              " 69,\n",
              " 188,\n",
              " 8,\n",
              " 30,\n",
              " 23,\n",
              " 7,\n",
              " 4,\n",
              " 249,\n",
              " 126,\n",
              " 93,\n",
              " 4,\n",
              " 114,\n",
              " 9,\n",
              " 2300,\n",
              " 1523,\n",
              " 5,\n",
              " 647,\n",
              " 4,\n",
              " 116,\n",
              " 9,\n",
              " 35,\n",
              " 8163,\n",
              " 4,\n",
              " 229,\n",
              " 9,\n",
              " 340,\n",
              " 1322,\n",
              " 4,\n",
              " 118,\n",
              " 9,\n",
              " 4,\n",
              " 130,\n",
              " 4901,\n",
              " 19,\n",
              " 4,\n",
              " 1002,\n",
              " 5,\n",
              " 89,\n",
              " 29,\n",
              " 952,\n",
              " 46,\n",
              " 37,\n",
              " 4,\n",
              " 455,\n",
              " 9,\n",
              " 45,\n",
              " 43,\n",
              " 38,\n",
              " 1543,\n",
              " 1905,\n",
              " 398,\n",
              " 4,\n",
              " 1649,\n",
              " 26,\n",
              " 6853,\n",
              " 5,\n",
              " 163,\n",
              " 11,\n",
              " 3215,\n",
              " 10156,\n",
              " 4,\n",
              " 1153,\n",
              " 9,\n",
              " 194,\n",
              " 775,\n",
              " 7,\n",
              " 8255,\n",
              " 11596,\n",
              " 349,\n",
              " 2637,\n",
              " 148,\n",
              " 605,\n",
              " 15358,\n",
              " 8003,\n",
              " 15,\n",
              " 123,\n",
              " 125,\n",
              " 68,\n",
              " 23141,\n",
              " 6853,\n",
              " 15,\n",
              " 349,\n",
              " 165,\n",
              " 4362,\n",
              " 98,\n",
              " 5,\n",
              " 4,\n",
              " 228,\n",
              " 9,\n",
              " 43,\n",
              " 36893,\n",
              " 1157,\n",
              " 15,\n",
              " 299,\n",
              " 120,\n",
              " 5,\n",
              " 120,\n",
              " 174,\n",
              " 11,\n",
              " 220,\n",
              " 175,\n",
              " 136,\n",
              " 50,\n",
              " 9,\n",
              " 4373,\n",
              " 228,\n",
              " 8255,\n",
              " 5,\n",
              " 25249,\n",
              " 656,\n",
              " 245,\n",
              " 2350,\n",
              " 5,\n",
              " 4,\n",
              " 9837,\n",
              " 131,\n",
              " 152,\n",
              " 491,\n",
              " 18,\n",
              " 46151,\n",
              " 32,\n",
              " 7464,\n",
              " 1212,\n",
              " 14,\n",
              " 9,\n",
              " 6,\n",
              " 371,\n",
              " 78,\n",
              " 22,\n",
              " 625,\n",
              " 64,\n",
              " 1382,\n",
              " 9,\n",
              " 8,\n",
              " 168,\n",
              " 145,\n",
              " 23,\n",
              " 4,\n",
              " 1690,\n",
              " 15,\n",
              " 16,\n",
              " 4,\n",
              " 1355,\n",
              " 5,\n",
              " 28,\n",
              " 6,\n",
              " 52,\n",
              " 154,\n",
              " 462,\n",
              " 33,\n",
              " 89,\n",
              " 78,\n",
              " 285,\n",
              " 16,\n",
              " 145,\n",
              " 95]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#our reviews are of different length, this is an issue, we must have them all of one same length\n",
        "#we will pad our sequences, if the review is greater than 250 words we will trim some extra words, on the contrary we will add\n",
        "\n",
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)"
      ],
      "metadata": {
        "id": "e8HRDmiLaxMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[1] #the 0's are due to the padding we applied"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYsuYAOqbT4x",
        "outputId": "8e9f1605-a0f8-4dcd-a2a9-e45b3e6d231e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     1,   194,\n",
              "        1153,   194,  8255,    78,   228,     5,     6,  1463,  4369,\n",
              "        5012,   134,    26,     4,   715,     8,   118,  1634,    14,\n",
              "         394,    20,    13,   119,   954,   189,   102,     5,   207,\n",
              "         110,  3103,    21,    14,    69,   188,     8,    30,    23,\n",
              "           7,     4,   249,   126,    93,     4,   114,     9,  2300,\n",
              "        1523,     5,   647,     4,   116,     9,    35,  8163,     4,\n",
              "         229,     9,   340,  1322,     4,   118,     9,     4,   130,\n",
              "        4901,    19,     4,  1002,     5,    89,    29,   952,    46,\n",
              "          37,     4,   455,     9,    45,    43,    38,  1543,  1905,\n",
              "         398,     4,  1649,    26,  6853,     5,   163,    11,  3215,\n",
              "       10156,     4,  1153,     9,   194,   775,     7,  8255, 11596,\n",
              "         349,  2637,   148,   605, 15358,  8003,    15,   123,   125,\n",
              "          68, 23141,  6853,    15,   349,   165,  4362,    98,     5,\n",
              "           4,   228,     9,    43, 36893,  1157,    15,   299,   120,\n",
              "           5,   120,   174,    11,   220,   175,   136,    50,     9,\n",
              "        4373,   228,  8255,     5, 25249,   656,   245,  2350,     5,\n",
              "           4,  9837,   131,   152,   491,    18, 46151,    32,  7464,\n",
              "        1212,    14,     9,     6,   371,    78,    22,   625,    64,\n",
              "        1382,     9,     8,   168,   145,    23,     4,  1690,    15,\n",
              "          16,     4,  1355,     5,    28,     6,    52,   154,   462,\n",
              "          33,    89,    78,   285,    16,   145,    95], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "UZujlV24bMYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJD8gBSibfdS",
        "outputId": "3955101d-eb06-45f6-f16e-98bdda08b9fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)\n",
        "\n",
        "#to accelerate the training: runtime > change > hardware accelerator gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTdVowDCb2NR",
        "outputId": "6f0fa176-78ed-45de-e6f2-c04117471dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 12s 11ms/step - loss: 0.4310 - acc: 0.7966 - val_loss: 0.3397 - val_acc: 0.8558\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2435 - acc: 0.9078 - val_loss: 0.3025 - val_acc: 0.8828\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.1830 - acc: 0.9329 - val_loss: 0.2782 - val_acc: 0.8956\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.1506 - acc: 0.9449 - val_loss: 0.3150 - val_acc: 0.8918\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.1259 - acc: 0.9546 - val_loss: 0.4253 - val_acc: 0.8454\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.1093 - acc: 0.9612 - val_loss: 0.3152 - val_acc: 0.8892\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.0965 - acc: 0.9685 - val_loss: 0.3186 - val_acc: 0.8744\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.0852 - acc: 0.9722 - val_loss: 0.3733 - val_acc: 0.8884\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.0741 - acc: 0.9766 - val_loss: 0.3281 - val_acc: 0.8878\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.0655 - acc: 0.9786 - val_loss: 0.3492 - val_acc: 0.8818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NOTE: Look for optimizers, like adam or rmsprop"
      ],
      "metadata": {
        "id": "zK2D54ESdnIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1ziXrqTdqKZ",
        "outputId": "19218b3e-dbf2-4ffa-a4ad-7729d06ab677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 5ms/step - loss: 0.4457 - acc: 0.8475\n",
            "[0.4456653594970703, 0.8475199937820435]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Making predictions\n",
        "\n",
        "#now that we trained our model, we will use it to make predictions on our own reviews.\n",
        "#Since our reviews are encoded well need to convert any review that we write into that form so the network can understand it. \n",
        "#To do that well load the encodings from the dataset and use them to encode our own data.\n",
        "\n",
        "word_index = imdb.get_word_index() #we can get the word index and print it\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJXo_t9yd76N",
        "outputId": "e31dab6c-36c3-4da9-d206-c7a020300565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# while were at it lets make a decode function\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0\n",
        "    text = \"\"\n",
        "    for num in integers:\n",
        "      if num != PAD:\n",
        "        text += reverse_word_index[num] + \" \"\n",
        "\n",
        "    return text[:-1]\n",
        "  \n",
        "print(decode_integers(encoded))\n",
        "\n",
        "#the encoded text gets decoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY1yQJrCeT70",
        "outputId": "229f0773-f779-469e-eb9a-1760c8d1c86d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that movie was just amazing so amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now time to make a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text) #we encode the text with the function above\n",
        "  pred = np.zeros((1,250)) #we create a blank numpy array in 0's, the shape that our model expects is at max 250\n",
        "  pred[0] = encoded_text #we insert our entry into the array\n",
        "  result = model.predict(pred) #we model the array\n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was so awesome! I really loved it and would watch it again because it was amazingly great\" #if we change a word, the difference of % will be significant\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7W-bkASecAR",
        "outputId": "f011b1e8-0611-435c-dfd9-bd4f2983268d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8169493]\n",
            "[0.2553257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#---RNN Play Generator---\n",
        "\n",
        "#this is the first NN that creates something for us, it will predict the next character in a sequence\n",
        "#we give it a sequence as an input and it will predict the next character\n",
        "#we train the model with a bunch of sequences from Romeo & Juliet\n",
        "\n",
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijtYxG_Jhc8U",
        "outputId": "229bc37a-9b56-4d87-ef84-c8370e667f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvCFH6yuiIQg",
        "outputId": "c03de7ad-2f1c-4aa8-99c7-8faee6fc3590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we can load our own text data from out computer, not neccesary in this example\n",
        "from google.colab import files \n",
        "path_to_file = list(files.upload().keys())[0]"
      ],
      "metadata": {
        "id": "tEYLEqMqiKrx",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "059f26af-4ea5-4c13-a131-2961ff521c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c8c6143c-658d-4dde-894a-2bc24d6b5cde\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c8c6143c-658d-4dde-894a-2bc24d6b5cde\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ff2d5305b39a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#we can load our own text data from out computer, not neccesary in this example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpath_to_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t77-XIwciOvT",
        "outputId": "3964694d-01df-41da-8102-5555df1bce89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6oE8ym1iV7n",
        "outputId": "94f03024-05f1-4728-9348-557e423f0f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding\n",
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "BYWNtXxaimp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW9D9buPiwUl",
        "outputId": "554b3a79-389f-40bf-e0ba-12011df9d353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to convert numeric values into text\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mZefMZri8xd",
        "outputId": "2a105317-6e64-460c-cda2-7cbee7aa454b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating training examples\n",
        "\n",
        "#the traning examples have input values of some length, we pick the length and then the output will be the exact same shifted by one character\n",
        "\n",
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) #converting the string dataset into characters"
      ],
      "metadata": {
        "id": "7SjObRbTjlmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "oTo3LRuNj0-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "FVjykk49j529"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of how it works\n",
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhOQZsdxkKkL",
        "outputId": "ee3ac7e5-faf9-407b-da22-2bee23f635fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training batches\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "heySCFfRkbfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model\n",
        "\n",
        "#this function returns a built model\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QItvOrfpklmm",
        "outputId": "2fd19f1c-56fc-4187-aed0-f912e7eb38fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a loss function to compile our model\n",
        "\n",
        "#the input to our model is of length 64, we feed it 64 training examples every time\n",
        "#we pass 64 entries of length 100 into the model as its training depth\n",
        "\n",
        "#we can use the model with the parameters that we saved, expecting a different input shape\n",
        "\n",
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsDB-LR-lLLx",
        "outputId": "7992a7f9-862a-4c02-cd4d-fe300e7f498c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdpo0pFtmLBo",
        "outputId": "5059407a-2545-4da9-a267-00d145c37b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-3.76761938e-03 -3.07332980e-03 -3.76729621e-03 ... -4.00916114e-03\n",
            "    5.18335216e-03  1.26092345e-03]\n",
            "  [-5.99067286e-03 -7.62989512e-04 -1.82354823e-03 ...  1.72426004e-03\n",
            "    6.11081440e-03  1.20752270e-03]\n",
            "  [-7.99202640e-03 -5.46373380e-03  1.03354244e-03 ...  5.12627419e-03\n",
            "    3.27634439e-03  6.92507951e-03]\n",
            "  ...\n",
            "  [ 6.61641592e-03 -3.18826106e-03  2.71500507e-03 ...  5.24223363e-03\n",
            "   -5.73351514e-03  4.15320741e-03]\n",
            "  [ 6.34859130e-03 -2.05716887e-03  2.41310592e-03 ...  1.49344560e-02\n",
            "   -7.77734257e-03 -9.99107491e-04]\n",
            "  [ 8.29135999e-03  3.31534026e-03 -6.78512035e-04 ...  1.21003063e-02\n",
            "   -1.19398562e-02 -2.35412712e-03]]\n",
            "\n",
            " [[-5.41017950e-03 -2.01345491e-03 -2.83408095e-03 ...  1.02606369e-04\n",
            "    1.32294116e-03 -5.61666675e-05]\n",
            "  [-2.95703241e-04 -3.83133581e-03 -2.16791546e-03 ...  6.62889739e-04\n",
            "   -7.55810831e-03 -3.52275372e-03]\n",
            "  [ 1.77477207e-03 -1.34114898e-03 -1.08763902e-03 ...  1.03139430e-02\n",
            "   -7.96142220e-03 -7.33126793e-03]\n",
            "  ...\n",
            "  [ 3.89058189e-03  3.66653712e-03 -6.35178201e-03 ... -3.35278129e-03\n",
            "    4.30141483e-03 -1.24656688e-03]\n",
            "  [-4.09489870e-03  2.69468362e-03 -7.48287793e-03 ... -1.42148975e-03\n",
            "   -4.82252566e-04 -5.65842167e-03]\n",
            "  [-1.03932284e-02  1.79088372e-03 -9.34846699e-03 ... -1.82160689e-03\n",
            "    7.24921119e-04 -1.02726677e-02]]\n",
            "\n",
            " [[ 1.28902937e-03 -3.51462699e-03  9.27525340e-04 ...  2.43813824e-03\n",
            "    1.87525060e-03 -9.16276244e-04]\n",
            "  [ 1.40815985e-03 -6.22239430e-03  2.94146105e-03 ...  4.92592389e-03\n",
            "    3.74508556e-04 -3.05294991e-04]\n",
            "  [ 2.95671425e-03 -5.52716665e-03  5.72742894e-03 ...  8.17131135e-04\n",
            "   -3.83907231e-03  8.27759271e-04]\n",
            "  ...\n",
            "  [ 8.03399179e-03 -1.09008490e-03  5.81478234e-05 ... -1.12530857e-03\n",
            "   -6.12318516e-03  1.77349872e-03]\n",
            "  [ 7.71104032e-03 -5.79992263e-03  2.02861289e-03 ... -7.50883017e-04\n",
            "   -3.66134336e-03  3.38448398e-03]\n",
            "  [ 8.95618927e-03 -2.65632547e-03 -1.37379253e-03 ...  6.27333997e-04\n",
            "   -7.19513744e-03  5.10439277e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.00091973e-03 -8.25963914e-04 -7.16478098e-05 ... -2.34065461e-04\n",
            "    1.49267749e-03  1.80104049e-03]\n",
            "  [ 4.21474921e-03 -2.82426039e-03  5.85573725e-04 ...  8.39786953e-04\n",
            "   -6.90339599e-03 -1.13189570e-03]\n",
            "  [-1.72770396e-03 -1.64649473e-03 -1.06782839e-03 ...  3.15514882e-03\n",
            "   -6.42208336e-03  3.86270997e-03]\n",
            "  ...\n",
            "  [-9.23118787e-05  3.23517481e-04 -3.35634965e-03 ...  2.88651092e-03\n",
            "   -4.98408498e-03  2.73133721e-03]\n",
            "  [ 1.38486072e-03  4.61432012e-03 -4.26073745e-03 ...  5.53059625e-03\n",
            "   -2.29207752e-03 -1.42786175e-03]\n",
            "  [ 2.52919504e-03  5.28257806e-03 -5.72729064e-03 ...  3.98162333e-03\n",
            "   -6.36858726e-03  2.53425399e-03]]\n",
            "\n",
            " [[ 1.82543416e-03  1.61468214e-03 -2.97346828e-03 ... -3.89343593e-04\n",
            "   -4.81890189e-03  2.90910807e-03]\n",
            "  [-3.11601488e-03  1.35140866e-03 -4.59241308e-03 ...  2.19830777e-03\n",
            "   -6.06231717e-03  5.85311977e-03]\n",
            "  [ 4.08642925e-04  2.35469453e-03 -2.86365161e-03 ...  1.18111735e-02\n",
            "   -6.89219264e-03 -5.81841916e-04]\n",
            "  ...\n",
            "  [ 3.10841203e-03 -3.40372510e-03 -8.41440167e-04 ...  1.13096945e-02\n",
            "    5.66662522e-03  6.37260824e-03]\n",
            "  [ 3.84041481e-03  1.74052874e-03 -1.64682651e-03 ...  1.31225958e-02\n",
            "    6.52669556e-03  1.51805289e-03]\n",
            "  [ 4.39226115e-03  3.07485391e-03 -3.18309991e-03 ...  1.06722591e-02\n",
            "    8.12124228e-04  5.05662709e-03]]\n",
            "\n",
            " [[ 3.33760586e-03 -2.63983384e-03  4.39827563e-04 ...  7.36072077e-04\n",
            "   -8.59473366e-03 -2.44208612e-03]\n",
            "  [ 4.55098366e-03  9.36012948e-05 -1.99461193e-03 ...  6.99424651e-04\n",
            "   -1.05495527e-02  2.06871168e-03]\n",
            "  [ 5.85093768e-03  2.15542037e-03 -4.07020841e-03 ...  1.30609330e-03\n",
            "   -1.24769472e-02  5.29166730e-03]\n",
            "  ...\n",
            "  [ 2.17813347e-03  7.50467181e-03  7.25849299e-04 ...  1.28045809e-02\n",
            "   -8.32293928e-03 -5.89928962e-03]\n",
            "  [ 6.45548943e-03  8.48697685e-03  6.86135236e-03 ...  1.25586074e-02\n",
            "   -2.65580881e-03 -4.01777914e-03]\n",
            "  [ 5.69532579e-03  8.37123301e-03  5.63667668e-03 ...  1.90523230e-02\n",
            "   -6.36266544e-03 -7.60755874e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-se0OsJxmO6C",
        "outputId": "dfb0dee8-c9c0-4aee-a41c-186f075e6920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00376762 -0.00307333 -0.0037673  ... -0.00400916  0.00518335\n",
            "   0.00126092]\n",
            " [-0.00599067 -0.00076299 -0.00182355 ...  0.00172426  0.00611081\n",
            "   0.00120752]\n",
            " [-0.00799203 -0.00546373  0.00103354 ...  0.00512627  0.00327634\n",
            "   0.00692508]\n",
            " ...\n",
            " [ 0.00661642 -0.00318826  0.00271501 ...  0.00524223 -0.00573352\n",
            "   0.00415321]\n",
            " [ 0.00634859 -0.00205717  0.00241311 ...  0.01493446 -0.00777734\n",
            "  -0.00099911]\n",
            " [ 0.00829136  0.00331534 -0.00067851 ...  0.01210031 -0.01193986\n",
            "  -0.00235413]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next\n",
        "#the output is the probability of every charachter in the next text"
      ],
      "metadata": {
        "id": "jsTii9ynmeDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677ce4d7-9995-4ca4-ba12-22d52fadbd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-3.7676194e-03 -3.0733298e-03 -3.7672962e-03  3.6641404e-03\n",
            " -4.3502627e-03 -5.1445952e-03  2.5555445e-04 -1.2329968e-03\n",
            "  2.4423338e-03  3.8808556e-03  3.7810891e-03  2.3661279e-03\n",
            " -4.9884301e-03  4.0323404e-03 -4.1584792e-03  2.2174872e-03\n",
            " -6.0416795e-03  9.1636612e-04  3.4224296e-03 -3.9064907e-04\n",
            " -2.0469795e-04  1.2818913e-03 -2.9246090e-03 -2.2688450e-04\n",
            "  3.7710308e-03 -8.1764709e-04  2.1036642e-03  1.3975715e-03\n",
            " -4.5382045e-03  3.0454807e-03  3.6413237e-03  2.7848615e-03\n",
            "  1.7147981e-04 -2.0448768e-03  2.7754391e-03  1.4950894e-03\n",
            " -4.3601356e-03 -6.3292717e-04 -4.8892183e-04  2.5127127e-04\n",
            "  6.8581628e-04 -5.0950167e-04 -9.0326800e-04  4.9525634e-03\n",
            " -4.2946122e-04  2.4405064e-03  6.7051128e-04 -4.3317676e-05\n",
            "  2.4606911e-03 -2.4021755e-03 -4.4638822e-03 -4.8655267e-03\n",
            "  1.3539928e-03 -2.2099223e-03 -8.2483393e-04 -4.9878294e-03\n",
            " -5.7647452e-03 -3.4344546e-03  9.7011626e-03 -1.8916623e-03\n",
            "  7.0176842e-03 -2.1972321e-03 -4.0091611e-03  5.1833522e-03\n",
            "  1.2609235e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1\n",
        "\n",
        "#the output is all of the predicted characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jl9nCtb2w_l4",
        "outputId": "cfab6e2f-a6bc-4a20-baf2-1936dae1a7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"CTfGUzOrNKNttwbOOgh\\nvt:y\\n,!ayMsbw\\nau-bHAk'elgUhLSqH$JVAS:FgsiMw:a3A:lZLc'!l?yBiw!fZGamOs.WBAs'w'BunH\""
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this takes all of the labels and logits (probability distributions) and computes a loss\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "mEHbYwwgxN6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---Compiling the Model---\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "w2Nx7oyKxrbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---Creating Checkpoints\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "drTrMiwrxyDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---Training---\n",
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSU4N2MVx4b4",
        "outputId": "f67b31d4-2147-4f05-8436-f4470b3991f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 14s 67ms/step - loss: 2.5751\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.8714\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.6267\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.4966\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.4181\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.3619\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.3164\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.2787\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.2430\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.2083\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.1727\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.1381\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 13s 66ms/step - loss: 1.1009\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.0623\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.0245\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.9834\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.9430\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.9029\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.8626\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.8258\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 13s 66ms/step - loss: 0.7895\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.7564\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.7239\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6955\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.6690\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6449\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.6234\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.6025\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5849\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5678\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.5543\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.5415\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.5304\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5177\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5066\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4975\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.4880\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4838\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.4768\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4711\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4631\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.4588\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4547\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.4499\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.4453\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4414\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4379\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4336\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.4318\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.4297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the more epochs, the better"
      ],
      "metadata": {
        "id": "6ngntAAVyFeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---Loading the model---\n",
        "\n",
        "#we rebuild the model with a batch size of 1\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "7BETpm3lyIW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the weights\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "ATO7B2rhyVfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we can load any checkpoint\n",
        "\n",
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "BHxXb-1T0CbR",
        "outputId": "3179ac5d-9292-4904-95b6-bcfb89e5da28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-378de0519873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpoint_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./training_checkpoints/ckpt_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mis_hdf5_filepath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m   return (filepath.endswith('.h5') or filepath.endswith('.keras') or\n\u001b[0m\u001b[1;32m    323\u001b[0m           filepath.endswith('.hdf5'))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.util._pywrap_checkpoint_reader.C' object has no attribute 'endswith'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#---Generating the text---\n",
        "\n",
        "#stolen from tensorflow\n",
        "\n",
        "#now we can use the tensorflow provided function\n",
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "n-6P_9CH0ML3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GcMIrDp0Yka",
        "outputId": "17f7e6f9-3d72-41d1-c0bd-19cc0b014d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: romeo\n",
            "romeould have all kindness\n",
            "And in their woes from breath, some day disged\n",
            "the heart of the music of the house of Lancaster.\n",
            "Your brother's son: hang him, and stand on sighs;\n",
            "But shall wo wive it shameless.\n",
            "\n",
            "LEONTES:\n",
            "Say you?\n",
            "\n",
            "Volsce:\n",
            "Peter.\n",
            "\n",
            "MENENIUS:\n",
            "Be else with me to this beard, than they--\n",
            "Myselves would offend along.\n",
            "\n",
            "Provost:\n",
            "Now, kiss me nobly\n",
            "To o'er-rough from me, and your emils have retired o' the foot;\n",
            "His eyes do prove a lady and more wife in honey.\n",
            "\n",
            "ROMEO:\n",
            "Why, she's your highness,--\n",
            "\n",
            "SICINIUS:\n",
            "He shall be of noble Pear i' the wars\n",
            "Since have but truth to thee well see, did vengeance for it!\n",
            "Shall I be master for his sword: he has, look'd upon:\n",
            "Come, peace, my lord, my gracious lords.\n",
            "\n",
            "EDWARD:\n",
            "But now you know the cause why music were as free to do; and, with a one and such provost\n"
          ]
        }
      ]
    }
  ]
}